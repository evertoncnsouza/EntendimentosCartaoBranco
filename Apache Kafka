O Kafka combina três recursos principais para que você possa implementar seus casos de
uso para streaming de eventos de ponta a ponta com uma única solução testada em batalha:

- Para publicar (gravar) e assinar (ler) fluxos de eventos, incluindo importação /
exportação contínua de seus dados de outros sistemas.
- Para armazenar fluxos de eventos de forma durável e confiável pelo tempo que você
quiser.
- Para processar fluxos de eventos conforme eles ocorrem ou retrospectivamente.

E toda essa funcionalidade é fornecida de maneira distribuída, altamente escalável,
elástica, tolerante a falhas e segura. O Kafka pode ser implantado em hardware
bare-metal, máquinas virtuais e contêineres, e no local, bem como na nuvem. 
Você pode escolher entre o autogerenciamento de seus ambientes Kafka e o uso de
serviços totalmente gerenciados oferecidos por uma variedade de fornecedores.

##########################################################################
Como funciona!

Kafka é um sistema distribuído que consiste em servidores e clientes que se comunicam
por meio de um protocolo de rede TCP de alto desempenho . Ele pode ser implantado em 
hardware bare-metal, máquinas virtuais e contêineres no local, bem como em ambientes 
de nuvem.

Servidores : o Kafka é executado como um cluster de um ou mais servidores que podem 
abranger vários datacenters ou regiões de nuvem. Alguns desses servidores formam a 
camada de armazenamento, chamados de corretores. Outros servidores executam o Kafka
Connect para importar e exportar dados continuamente como fluxos de eventos para 
integrar o Kafka com seus sistemas existentes, como bancos de dados relacionais, 
bem como outros clusters Kafka. Para permitir que você implemente casos de uso de 
missão crítica, um cluster Kafka é altamente escalonável e tolerante a falhas: se 
algum de seus servidores falhar, os outros servidores assumirão seu trabalho para 
garantir operações contínuas sem nenhuma perda de dados.

Clientes : eles permitem que você escreva aplicativos e microsserviços distribuídos
que leem, gravam e processam fluxos de eventos em paralelo, em escala e de maneira
tolerante a falhas, mesmo no caso de problemas de rede ou falhas de máquina. 
O Kafka vem com alguns desses clientes incluídos, que são aumentados por dezenas de 
clientes fornecidos pela comunidade Kafka: os clientes estão disponíveis para Java e
Scala, incluindo a biblioteca Kafka Streams de nível superior , para Go, Python, C /
C ++ e muitas outras programações linguagens, bem como APIs REST.

Principais conceitos e terminologia
Um evento registra o fato de que "algo aconteceu" no mundo ou no seu negócio. 
Também é chamado de registro ou mensagem na documentação. Ao ler ou gravar dados no
Kafka, você o faz na forma de eventos. Conceitualmente, um evento possui uma chave,
um valor, um carimbo de data / hora e cabeçalhos de metadados opcionais. 
Aqui está um exemplo de evento:

Chave do evento: "Alice"
Valor do evento: "Efetuou um pagamento de $ 200 para Bob"
Data e hora do evento: "25 de junho de 2020 às 14h06"
Os produtores são os aplicativos clientes que publicam (gravam) eventos no Kafka e
os consumidores são os que assinam (lêem e processam) esses eventos.
Em Kafka, produtores e consumidores são totalmente dissociados e agnósticos entre si,
o que é um elemento chave de design para alcançar a alta escalabilidade pela qual 
Kafka é conhecida. Por exemplo, os produtores nunca precisam esperar pelos 
consumidores. O Kafka oferece várias garantias , como a capacidade de processar 
eventos exatamente uma vez.

Os eventos são organizados e armazenados de forma duradoura em tópicos . 
Muito simplificado, um tópico é semelhante a uma pasta em um sistema de arquivos,
e os eventos são os arquivos dessa pasta. Um exemplo de nome de tópico poderia ser
"pagamentos". Os tópicos no Kafka são sempre multiprodutor e multi-assinante: um 
tópico pode ter zero, um ou muitos produtores que gravam eventos nele, bem como zero,
um ou muitos consumidores que assinam esses eventos. Os eventos em um tópico podem 
ser lidos com a frequência necessária - ao contrário dos sistemas de mensagens
tradicionais, os eventos não são excluídos após o consumo. Em vez disso, você define
por quanto tempo o Kafka deve reter seus eventos por meio de uma definição de 
configuração por tópico, após o qual os eventos antigos serão descartados. 
O desempenho do Kafka é efetivamente constante com relação ao tamanho dos dados, 
portanto, armazenar dados por um longo tempo é perfeitamente adequado.

Os tópicos são particionados , o que significa que um tópico é espalhado por vário
s "depósitos" localizados em diferentes corretores Kafka. Esse posicionamento
distribuído de seus dados é muito importante para a escalabilidade, pois permite que
os aplicativos clientes leiam e gravem os dados de / para vários corretores ao mesmo
tempo. Quando um novo evento é publicado em um tópico, ele na verdade é anexado a uma
das partições do tópico. Eventos com a mesma chave de evento (por exemplo, 
um cliente ou ID de veículo) são gravados na mesma partição, e Kafka garante que
qualquer consumidor de uma determinada partição de tópico sempre lerá os eventos
dessa partição exatamente na mesma ordem em que foram gravados.

#########################################################

APIs Kafka
Além das ferramentas de linha de comando para tarefas de gerenciamento e 
administração, o Kafka tem cinco APIs principais para Java e Scala:

A API Admin para gerenciar e inspecionar tópicos, corretores e outros objetos Kafka.
A API do Produtor para publicar (gravar) um fluxo de eventos para um ou mais tópicos
do Kafka.
A API do consumidor para assinar (ler) um ou mais tópicos e processar o fluxo de
eventos produzidos para eles.
A API Kafka Streams para implementar aplicativos de processamento de fluxo e 
microsserviços. Ele fornece funções de nível superior para processar fluxos de
eventos, incluindo transformações, operações com estado como agregações e junções,
janelas, processamento com base no horário do evento e muito mais. A entrada é lida 
de um ou mais tópicos a fim de gerar saída para um ou mais tópicos, transformando 
efetivamente os fluxos de entrada em fluxos de saída.
A API do Kafka Connect para construir e executar conectores de importação / 
exportação de dados reutilizáveis que consomem (lêem) ou produzem (gravam) fluxos 
de eventos de e para sistemas e aplicativos externos, para que possam se integrar ao 
Kafka. Por exemplo, um conector para um banco de dados relacional como PostgreSQL 
pode capturar todas as alterações em um conjunto de tabelas. No entanto, na prática,
você normalmente não precisa implementar seus próprios conectores porque a comunidade
Kafka já fornece centenas de conectores prontos para uso.

